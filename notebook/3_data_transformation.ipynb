{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68985170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd94016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\kk061\\\\OneDrive\\\\Desktop\\\\python\\\\Mlflow Main\\\\Heart Stroke Prediction\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8c0182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\kk061\\\\OneDrive\\\\Desktop\\\\python\\\\Mlflow Main\\\\Heart Stroke Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83b7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a176599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-09 02:39:04,435: INFO: __init__: This is an info message.]\n",
      "[2025-04-09 02:39:04,437: ERROR: __init__: This is an error message.]\n"
     ]
    }
   ],
   "source": [
    "from src.heartstrokeprediction.constants import *\n",
    "from src.heartstrokeprediction.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0398f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf2100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.heartstrokeprediction import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "653ff8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation: \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "\n",
    "        # Drop the 'id' column from the dataset if it exists\n",
    "        if 'id' in data.columns:\n",
    "            data.drop(columns=['id'], inplace=True)\n",
    "            logger.info(\"Dropped 'id' column from the dataset\")\n",
    "\n",
    "        # Calculate the mean value of the 'bmi' column and fill NaN values with the mean\n",
    "        mean_value = data['bmi'].mean()\n",
    "        data['bmi'].fillna(mean_value, inplace=True)\n",
    "\n",
    "        # Apply label encoding to categorical columns\n",
    "        label_encoder = LabelEncoder()\n",
    "        data['gender'] = label_encoder.fit_transform(data['gender'])\n",
    "        data['ever_married'] = label_encoder.fit_transform(data['ever_married'])\n",
    "        data['work_type'] = label_encoder.fit_transform(data['work_type'])\n",
    "        data['Residence_type'] = label_encoder.fit_transform(data['Residence_type'])\n",
    "        data['smoking_status'] = label_encoder.fit_transform(data['smoking_status'])\n",
    "        \n",
    "        logger.info(\"Applied label encoding to categorical columns\")\n",
    "\n",
    "        # Split the data into features and target (assuming the target is the last column)\n",
    "        X = data.drop(columns=['stroke'])\n",
    "        y = data['stroke']\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Save the transformed data to CSV files\n",
    "        train_transformed = pd.concat([X_train, y_train], axis=1)\n",
    "        test_transformed = pd.concat([X_test, y_test], axis=1)\n",
    "        \n",
    "        train_transformed.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "        test_transformed.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)\n",
    "\n",
    "        logger.info(\"Splitted and transformed data into training and test sets\")\n",
    "        logger.info(f\"Train shape: {train_transformed.shape}\")\n",
    "        logger.info(f\"Test shape: {test_transformed.shape}\")\n",
    "\n",
    "        print(f\"Train shape: {train_transformed.shape}\")\n",
    "        print(f\"Test shape: {test_transformed.shape}\")\n",
    "        print(f\"Train target column 'stroke' has {y_train.isnull().sum()} NaN values.\")\n",
    "        print(f\"Test target column 'stroke' has {y_test.isnull().sum()} NaN values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29bf1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-09 02:49:08,519: INFO: common: YAML file: config\\config.yaml loaded successfully]\n",
      "[2025-04-09 02:49:08,522: INFO: common: YAML file: params.yaml loaded successfully]\n",
      "[2025-04-09 02:49:08,526: INFO: common: YAML file: schema.yaml loaded successfully]\n",
      "[2025-04-09 02:49:08,530: INFO: common: created directory at: artifacts]\n",
      "[2025-04-09 02:49:08,532: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-04-09 02:49:08,545: INFO: 3276256660: Dropped 'id' column from the dataset]\n",
      "[2025-04-09 02:49:08,555: INFO: 3276256660: Applied label encoding to categorical columns]\n",
      "[2025-04-09 02:49:08,584: INFO: 3276256660: Splitted and transformed data into training and test sets]\n",
      "[2025-04-09 02:49:08,585: INFO: 3276256660: Train shape: (3577, 11)]\n",
      "[2025-04-09 02:49:08,586: INFO: 3276256660: Test shape: (1533, 11)]\n",
      "Train shape: (3577, 11)\n",
      "Test shape: (1533, 11)\n",
      "Train target column 'stroke' has 0 NaN values.\n",
      "Test target column 'stroke' has 0 NaN values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kk061\\AppData\\Local\\Temp\\ipykernel_6784\\3276256660.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['bmi'].fillna(mean_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.train_test_spliting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d893b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac15ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055221c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd81f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
